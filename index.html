<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Model?">
  <meta name="keywords" content="Spatial Reasoning, Point Cloud, Large Language Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Model?</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Model?</h1>
<!--           <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/WeichenZh">Weichen Zhang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="">Ruiying Peng</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://fi.ee.tsinghua.edu.cn/~gaochen/">Chen Gao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Jianjie Fang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="">Xin Zeng</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="">Kaiyuan Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Ziyou Wang</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="">Jinqiang Cui</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Xin Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://chen-xinlei.com/">Xinlei Chen</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://fi.ee.tsinghua.edu.cn/~liyong/">Yong Li</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua Univeristy,</span>
            <span class="author-block"><sup>2</sup>Pengcheng laboratory,</span>
            <span class="author-block"><sup>3</sup>Northeastern University,</span>
            <span class="author-block"><sup>4</sup>Sun Yat-sen University</span>
          </div>
 -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://anonymous.4open.science/r/3D-point-of-attention-7912/README.md"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://anonymous.4open.science/r/3D-point-of-attention-7912/README.md"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D Large Language Models (LLMs) leveraging spatial information in point clouds for 3D spatial 
            reasoning attract great attention. Despite some promising results, the role of point clouds in 3D 
            spatial reasoning remains under-explored. In this work, we comprehensively evaluate and analyze these 
            models to answer the research question: <i>Does point cloud truly boost the spatial reasoning 
            capacities of 3D LLMs?</i> We first evaluate the spatial reasoning capacity of LLMs with different 
            input modalities by replacing the point cloud with the visual and text counterparts. We then propose 
            a novel 3D QA (Question-answering) benchmark, ScanReQA, that comprehensively evaluates models' understanding 
            of binary spatial relationships. Our findings reveal several critical insights: 1) LLMs without point input 
            could even achieve competitive performance even in a zero-shot manner; 2) existing 3D LLMs struggle to 
            comprehend the binary spatial relationships; 3) 3D LLMs exhibit limitations in exploiting the structural 
            coordinates in point clouds for fine-grained spatial reasoning. We think these conclusions can help the 
            next step of 3D LLMs and also offer insights for foundation models in other modalities.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>


<section class="section">
    <div class="container is-max-desktop">

      <!-- Teaser -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Overview</h2>

          <div class="content has-text-justified">
            <p>
            <b>The evaluation overview of the 3D LLMs.</b> In the Multi-modal Input Evaluation part, they are tested on 3D QA tasks with the point cloud, 
            images, and text description inputs. Both images and text descriptions are derived from the original scene point clouds, 
            including necessary information to answer the 3D QA. In the Spatial Reasoning Evaluation, a novel 3D QA benchmark is proposed to evaluate 
            3D LLMs reasoning capacities for relative spatial relationships and absolute spatial coordinates. 
            </p>
          </div>
          <div style="width: 100%; margin: 0 auto;">
              <img src="./static/images/cover_page.jpg" width="100%">
          </div>

        </div>
      </div>
      <!--/ Data -->
    </div>
  </section>
  
<section class="section">
    <div class="container is-max-desktop">

      <!-- Teaser -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Multi-modal Input Generation</h2>
          <div class="content has-text-justified">
            <p>
            The multi-modal input data is generated from the original scene point cloud. The point clouds are projected into the continuous RGB frames and 
            then uniformly downsample to fixed-length frames as the VLM's RGB input. The RGB frames are further captioned with off-the-shelf model to generate 
            scene captions as the LLM's input. 
            </p>
          </div>
          <div style="width: 100%; margin: 0 auto;">
              <img src="./static/images/data_gen_page.jpg" width="100%">
          </div>
        </div>
      </div>
      <!--/ Data -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">

      <!-- Teaser -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">ScanReQA Generation</h2>
          <div class="content has-text-justified">
            <p>
            ScanReQA includes RelSpatialQA and AbsSpatialQA, which evaluate 3D LLMs' reasoning capacities for relative spatial relationships and absolute 3D coordinates. 
            RelSpatialQA is generated from the forward and backward spatial relationship triplets, requiring the model to understand the binary spatial relationships from 
            two opposite perspectives. AbsSpatialQA is derived from the 3D visual grounding tasks, but is more challenging. The model not only need to reason for the referred 
            object, but also choose the object coordination from 4 options. 
            </p>
          </div>
          <div style="width: 100%; margin: 0 auto;">
              <img src="./static/images/qa_gen_v2.jpg" width="100%">
          </div>
        </div>
      </div>
      <!--/ Data -->
    </div>
  </section>


    <section class="section">
    <div class="container is-max-desktop">

      <!-- Teaser -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiment</h2>
          <div class="content has-text-justified">
            <p>
            <b>EM@1 on ScanQA, SQA3D and RelSpatialQA</b>. 3D LLMs fail to outperform other models with vision-only or text-only inputs. The zero-shot LLMs achieve the 
            best results on RelSpatialQA while the fine-tuned VLMs outperform other methods on ScanQA and SQA3D. In the table, all models have significantly low accuracies in binary 
            spatial relationship reasoning and absolute coordinate prediction.
            </p>
          </div>
          <div style="width: 100%; margin: 0 auto;">
              <img src="./static/images/em_multi_modal_page.jpg" width="100%">
          </div>
        </div>
      </div>
      <!--/ Data -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
    <!-- Model -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-5">Accurary on ScanReQA</h3>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            <b>Accuracy and recall on ScanReQA</b>
            (1) Overall accuracy and recall are extremely low, at only 8.3% and 12.7%, respectively; 
            (2) LLMs achieve the highest accuracy while zero-shot VLMs achieve the lowest, with 3D LLMs falling between them;
            (3) Most 3D LLMs achieve near-zero accuracy on AbsSpatialQA. Even the best-performing model, 3D-LLM, only reaches the accuracy of 24.2%, 
            which is lower than random guessing (25%).
          </p>
        </div>
      </div>
      <div class="column">
          <img src="./static/images/acc_ScanReQA.jpg" width="100%">
      </div>
    </div>
    <!--/ Model -->
    
  </div>
</section>


    <section class="section">
    <div class="container is-max-desktop">

      <!-- Teaser -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Ablation Study</h2>
          <div class="content has-text-justified">
          <p>
            <b>Ablations of Multi-modal Combinations</b>. 3D LLMs with multi-view images input are more sensitive to 
            visual modality while those with RGB point clouds are more affected by the point cloud modality. 
            The results of Chat-Scene and Chat-3D demonstrate that the model solely relies on text input could 
            even outperform multimodal input on 3D spatial reasoning tasks. All 3D LLMs, except 3D-LLM, achieve higher accuracy 
            with PVTI than with PVI, indicating that redundancy in the text modality enhances spatial reasoning 
            performance, supporting the extension of this conclusion from VLMs to 3D LLMs.
          </p>
          </div>
          <div style="width: 100%; margin: 0 auto;">
              <img src="./static/images/multi_modal_ablation.jpg" width="100%">
          </div>
        </div>
      </div>
      <!--/ Data -->
    </div>
  </section>
  

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code></code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website template adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
